{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from /homedata/mcarenso/shear/SAM3d_Tropics/var_id_days_i_t.json\n",
      "Loading storms...\n",
      "loading storms from netcdf\n",
      "Warning: ecCodes 2.21.0 or higher is recommended. You are running version 2.16.0\n",
      "Time elapsed for loading storms: 1.76 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "# plt.xkcd()\n",
    "# rcParams['font.family'] = ['xkcd', 'Comic Neue', 'Comic Mono']\n",
    "\n",
    "import xarray as xr\n",
    "import random \n",
    "import os \n",
    "\n",
    "from work import handler\n",
    "from work import casestudy\n",
    "from work import storm_tracker\n",
    "\n",
    "from work.plots.hist import simple_hist\n",
    "from work.transect import add_transects_with_aligned_boxes,make_mask_box\n",
    "\n",
    "settings_path = 'settings/sam3d.yaml'\n",
    "\n",
    "# import matplotlib.cm as cm\n",
    "# from scipy.interpolate import CloughTocher2DInterpolator, LinearNDInterpolator, NearestNDInterpolator\n",
    "# import glob\n",
    "# import intake\n",
    "# import dask\n",
    "# import functools\n",
    "# import pandas as pd\n",
    "# dask.config.set({\"array.slicing.split_large_chunks\": True}) \n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "# import cmocean\n",
    "# # !pip install easygems\n",
    "# import tqdm\n",
    "# import scipy\n",
    "# import datetime as dt \n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# from funcs import *\n",
    "\n",
    "\n",
    "hdlr = handler.Handler(settings_path)\n",
    "cs = casestudy.CaseStudy(hdlr, overwrite = False ,verbose = False)\n",
    "st = storm_tracker.StormTracker(cs, overwrite_storms = False, overwrite = False, verbose = True) #overwrite = True is super long, computes growth rate (triangle fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_min = 6  # or 10\n",
    "surfmaxkm2_min = 10000  # or other value\n",
    "region_latmin, region_latmax, region_lonmin, region_lonmax = -15, 30, -180, 180\n",
    "filename_save = f\"profile_dataset_storms_dmin{duration_min}_smin{surfmaxkm2_min}_lat{region_latmin}_{region_latmax}_lon{region_lonmin}_{region_lonmax}.nc\"\n",
    "storms_path = os.path.join(st.settings[\"DIR_DATA_OUT\"], cs.name, filename_save)\n",
    "ds = xr.open_dataset(storms_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABS_init_profile = ds['TABS_init_profile'].values  # Shape: (num_samples, num_levels)\n",
    "QV_init_profile = ds['QV_init_profile'].values\n",
    "U_init_profile = ds['U_init_profile'].values\n",
    "V_init_profile = ds['V_init_profile'].values\n",
    "\n",
    "TABS_max_instant_profile = ds['TABS_max_instant_profile'].values\n",
    "QV_max_instant_profile = ds['QV_max_instant_profile'].values\n",
    "U_max_instant_profile = ds['U_max_instant_profile'].values\n",
    "V_max_instant_profile = ds['V_max_instant_profile'].values\n",
    "\n",
    "# Extract scalar variables\n",
    "lon_init = ds['lon_init'].values  # Shape: (num_samples,)\n",
    "lat_init = ds['lat_init'].values\n",
    "time_init = pd.to_datetime(ds['time_init'].values)\n",
    "\n",
    "lon_max_instant = ds['lon_max_instant'].values\n",
    "lat_max_instant = ds['lat_max_instant'].values\n",
    "time_max_instant = pd.to_datetime(ds['time_max_instant'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time variables to Unix timestamp (seconds since epoch)\n",
    "time_init_numeric = time_init.astype(np.int64) // 10**9\n",
    "time_max_instant_numeric = time_max_instant.astype(np.int64) // 10**9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples and levels\n",
    "num_samples = TABS_init_profile.shape[0]\n",
    "num_levels = TABS_init_profile.shape[1]\n",
    "\n",
    "# Concatenate profile variables\n",
    "profile_vars_init = [TABS_init_profile, QV_init_profile, U_init_profile, V_init_profile]\n",
    "profile_vars_max = [TABS_max_instant_profile, QV_max_instant_profile, U_max_instant_profile, V_max_instant_profile]\n",
    "\n",
    "# Concatenate initial and max instant profiles along the feature axis\n",
    "profiles_init_concat = np.concatenate(profile_vars_init, axis=1)  # Shape: (num_samples, 4 * num_levels)\n",
    "profiles_max_concat = np.concatenate(profile_vars_max, axis=1)    # Shape: (num_samples, 4 * num_levels)\n",
    "\n",
    "# Reshape scalar variables to 2D arrays\n",
    "lon_init = lon_init.reshape(-1, 1)\n",
    "lat_init = lat_init.reshape(-1, 1)\n",
    "time_init_numeric = time_init_numeric.reshape(-1, 1)\n",
    "\n",
    "lon_max_instant = lon_max_instant.reshape(-1, 1)\n",
    "lat_max_instant = lat_max_instant.reshape(-1, 1)\n",
    "time_max_instant_numeric = time_max_instant_numeric.reshape(-1, 1)\n",
    "\n",
    "# Concatenate scalar variables\n",
    "scalar_vars = np.hstack([\n",
    "    lon_init, lat_init, time_init_numeric,\n",
    "    lon_max_instant, lat_max_instant, time_max_instant_numeric\n",
    "])  # Shape: (num_samples, 6)\n",
    "\n",
    "# Combine all inputs\n",
    "X = np.hstack([profiles_init_concat, profiles_max_concat, scalar_vars])  # Shape: (num_samples, total_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Number of profile features\n",
    "num_profile_features = profiles_init_concat.shape[1] + profiles_max_concat.shape[1]  # 8 * num_levels\n",
    "\n",
    "# Split profiles and scalars\n",
    "X_profiles = X[:, :num_profile_features]\n",
    "X_scalars = X[:, num_profile_features:]\n",
    "\n",
    "# Normalize profiles using StandardScaler\n",
    "scaler_profiles = StandardScaler()\n",
    "X_profiles_scaled = scaler_profiles.fit_transform(X_profiles)\n",
    "\n",
    "# Normalize scalars using MinMaxScaler\n",
    "scaler_scalars = MinMaxScaler()\n",
    "X_scalars_scaled = scaler_scalars.fit_transform(X_scalars)\n",
    "\n",
    "# Combine scaled profiles and scalars\n",
    "X_scaled = np.hstack([X_profiles_scaled, X_scalars_scaled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input_dim = X_train.shape[1]  # Total number of features\n",
    "\n",
    "# Input layer\n",
    "input_layer = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoded = layers.Dense(512, activation='relu')(input_layer)\n",
    "encoded = layers.Dense(256, activation='relu')(encoded)\n",
    "encoded = layers.Dense(128, activation='relu')(encoded)\n",
    "bottleneck = layers.Dense(64, activation='relu')(encoded)  # Bottleneck layer\n",
    "\n",
    "# Decoder\n",
    "decoded = layers.Dense(128, activation='relu')(bottleneck)\n",
    "decoded = layers.Dense(256, activation='relu')(decoded)\n",
    "decoded = layers.Dense(512, activation='relu')(decoded)\n",
    "output_layer = layers.Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, X_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Autoencoder Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = keras.Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "# Encode the data\n",
    "encoded_data = encoder.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('autoencoder_model.h5')\n",
    "encoder.save('encoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct inputs\n",
    "reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "# Compare original and reconstructed data for a sample\n",
    "sample_index = 0  # Change as needed\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(X_test[sample_index], label='Original')\n",
    "plt.plot(reconstructed_data[sample_index], label='Reconstructed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Chat separate encoding\n",
    "\n",
    "# # Profile input\n",
    "# input_profiles = keras.Input(shape=(num_profile_features,))\n",
    "# x_profiles = layers.Dense(256, activation='relu')(input_profiles)\n",
    "# x_profiles = layers.Dense(128, activation='relu')(x_profiles)\n",
    "\n",
    "# # Scalar input\n",
    "# input_scalars = keras.Input(shape=(X_scalars_scaled.shape[1],))\n",
    "# x_scalars = layers.Dense(32, activation='relu')(input_scalars)\n",
    "\n",
    "# # Combine profiles and scalars\n",
    "# combined = layers.concatenate([x_profiles, x_scalars])\n",
    "\n",
    "# # Bottleneck layer\n",
    "# bottleneck = layers.Dense(64, activation='relu')(combined)\n",
    "\n",
    "# # Decoder for combined data\n",
    "# x = layers.Dense(128, activation='relu')(bottleneck)\n",
    "# x = layers.Dense(256, activation='relu')(x)\n",
    "# output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "# # Autoencoder model with two inputs\n",
    "# autoencoder = keras.Model(inputs=[input_profiles, input_scalars], outputs=output_layer)\n",
    "\n",
    "# # Compile the model\n",
    "# autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# autoencoder.summary()\n",
    "\n",
    "# # Train the model\n",
    "# history = autoencoder.fit(\n",
    "#     [X_train[:, :num_profile_features], X_train[:, num_profile_features:]], X_train,\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     shuffle=True,\n",
    "#     validation_data=([X_test[:, :num_profile_features], X_test[:, num_profile_features:]], X_test)\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyHD)",
   "language": "python",
   "name": "pyhd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
